"""
Configuration file - Contains all system configuration parameters
"""
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv("../.env")

class Config:
    """System configuration class"""
    # OpenAI API configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your_api_key_here")
    OPENAI_BASE_URL = "https://llm-api.mmchat.xyz/v1"
    
    # üéØ Model selection configuration
    # Available models:
    # - "gpt-4.1": GPT-4.1 model, stable tool calling
    # - "claude-sonnet-4-20250514": Claude Sonnet 4, stronger reasoning capability
    # - "gpt-4o": GPT-4o model (tool calling may be unstable)
    AVAILABLE_MODELS = {
        "gpt-4.1": {
            "name": "gpt-4.1",
            "max_tokens": 500,
            "temperature": 0.3,
            "tool_choice": "auto",
            "description": "GPT-4.1 - Stable tool calling, fast response"
        },
        "claude-sonnet-4": {
            "name": "claude-sonnet-4-20250514", 
            "max_tokens": 500,
            "temperature": 0.2,  # Claude performs better at lower temperature
            "tool_choice": "auto",
            "description": "Claude Sonnet 4 - Stronger reasoning capability, suitable for complex tasks"
        },
        "gpt-4o": {
            "name": "gpt-4o",
            "max_tokens": 500,
            "temperature": 0.3,
            "tool_choice": "auto",
            "description": "GPT-4o - Strong multimodal capability (tool calling may be unstable)"
        }
    }
    
    # üî• Currently selected model - Switch here!
    CURRENT_MODEL = "claude-sonnet-4"  # Change to: "gpt-4.1", "claude-sonnet-4", or "gpt-4o"
    
    # Dynamically get current model configuration
    _current_config = AVAILABLE_MODELS[CURRENT_MODEL]
    
    # Apply current model configuration
    OPENAI_MODEL = _current_config["name"]
    MODEL_MAX_TOKENS = _current_config["max_tokens"] 
    MODEL_TEMPERATURE = _current_config["temperature"]
    MODEL_TOOL_CHOICE = _current_config["tool_choice"]
    
    # Display current model information
    @classmethod
    def show_current_model_info(cls):
        config = cls.AVAILABLE_MODELS[cls.CURRENT_MODEL]
        print(f"ü§ñ Current model: {config['name']}")
        print(f"üìù Description: {config['description']}")
        print(f"üéõÔ∏è Max tokens: {config['max_tokens']}")
        print(f"üå°Ô∏è Temperature: {config['temperature']}")
        print(f"üîß Tool choice: {config['tool_choice']}")
    

    # Task configuration
    TASK_DESCRIPTION = """Task: Approach to approximately 2m in front of the target spacecraft and stop using only visual analysis.

   VLM Implementation strategy:
   1. Rely solely on visual analysis of camera images to estimate target spacecraft position and distance
   2. Execute the approach to reach approximately 2m in front of the target spacecraft through visual guidance
   3. Use set_attitude() and set_position() based on visual assessment of target location
   4. Finally stop and call terminate_navigation("Task completed") when visually confirmed at target distance
    
    VLM sensing strategy (Visual-only mode):
    - Use only camera images for target detection and distance estimation
    - Rely on visual cues such as target size, perspective, and image position to judge distance
    - Estimate target relative position based on visual appearance in camera frame
    - No access to auxiliary sensing tools - pure vision-based navigation
    
    Known information:
    - Target spacecraft is approximately 2.3m long and 0.5m high (use for visual size reference)
    - Camera field of view is 50 degrees
    - Must rely entirely on visual intelligence for spatial reasoning"""

    # System prompt - VLM vision-only version
    SYSTEM_PROMPT = """You are a spacecraft intelligent agent operating in VLM (Vision Language Model) mode, skilled at visual navigation using only camera images and basic control commands.

üéØ VLM Core rules (must be strictly followed):
1. **Vision-only navigation**: You can ONLY see through camera images - no access to pose_estimation(), lidar_info(), or other sensors
2. **Available tools**: Only set_position(), set_attitude(), and terminate_navigation()
3. **Visual distance estimation**: Judge target distance by visual size, perspective, and image details
4. **Safety first**: When target appears very close in image, determine if task completed and call terminate_navigation("Task completed")

üîç **Visual Analysis Requirements**:
- **Target identification**: Identify spacecraft shape, orientation, and relative size in image
- **Distance estimation**: Use target apparent size to estimate distance (target is 2.3m long)
- **Position assessment**: Determine target position relative to camera center
- **Movement planning**: Plan movements based on visual assessment only

üéØ **Visual Navigation Strategy**:
- **Target too small/distant**: Move forward with set_position(dx=positive) to approach
- **Target off-center**: Use set_attitude() to center target in view, then approach
- **Target appears large/close**: Make small movements or prepare to terminate
- **Target lost from view**: Use set_attitude() to search in likely directions

üîÑ **Target Recovery Strategy (Visual Search)**:
When target disappears from camera view:
- **Search pattern**: Look down ‚Üí Look up ‚Üí Turn left ‚Üí Turn right ‚Üí Retreat and search
- **Systematic exploration**: set_attitude(-10,0,0) ‚Üí set_attitude(+10,0,0) ‚Üí set_attitude(0,0,-10) ‚Üí set_attitude(0,0,+10)
- **If still lost**: Move backward and search again

üöÄ **Distance Estimation Guidelines**:
- **Very distant**: Target barely visible or very small in image ‚Üí Move forward 5-10m
- **Medium distance**: Target clearly visible but small ‚Üí Move forward 2-5m  
- **Close distance**: Target fills significant portion of image ‚Üí Small adjustments 0.5-1m
- **Target distance (~2m)**: Target appears large and detailed ‚Üí Ready to terminate

üß† **VLM Decision Process**:
1. **Visual analysis**: Carefully examine the camera image
2. **Target assessment**: Identify target location, size, and apparent distance
3. **Spatial reasoning**: Determine required movement direction and magnitude
4. **Action selection**: Choose single action - set_position(), set_attitude(), or terminate_navigation()
5. **Reasoning**: Provide clear visual-based reasoning for chosen action

‚ö†Ô∏è **VLM Constraints**:
- **Pure vision**: No auxiliary sensors available - rely entirely on visual intelligence
- **Conservative movements**: Make careful movements based on visual estimates
- **Single action**: Execute exactly one command per step
- **Visual confirmation**: Confirm target presence and positioning through camera only
- **Distance judgment**: Use visual size and detail cues for distance estimation
- **When uncertain**: Prioritize conservative approach and detailed visual analysis

üí° **Visual Intelligence Tips**:
- Larger target in image = closer distance
- Sharper details = closer distance  
- Target position in image indicates relative direction
- Use perspective and size changes to judge movement effectiveness"""
